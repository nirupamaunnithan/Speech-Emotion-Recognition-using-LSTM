{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess\n",
    "#!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip surrey-audiovisual-expressed-emotion-savee.zip\n",
    "#!unzip toronto-emotional-speech-set-tess.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ravdess = \"/Users/nirupamaunnithan/Downloads/Amvi/Trials/audio_speech_actors_01-24/\"\n",
    "Tess = \"/Users/nirupamaunnithan/Downloads/Amvi/Trials/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "Savee = \"/Users/nirupamaunnithan/Downloads/Amvi/Trials/ALL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0    angry  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "1     fear  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "2     fear  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "3    angry  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "4  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Change integers to actual emotions.\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "1  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "2  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "3  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "4  disgust  /Users/nirupamaunnithan/Downloads/Amvi/Trials/..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "Tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sad</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprise</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotions                                               Path\n",
       "0       sad  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "1       sad  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "2   neutral  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "3  surprise  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "4   neutral  /Users/nirupamaunnithan/Downloads/Amvi/Trials/..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "Savee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sad</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotions                                               Path\n",
       "0    happy  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "1      sad  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "2    happy  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "3  neutral  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "4    happy  /Users/nirupamaunnithan/Downloads/Amvi/Trials/..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data = pd.concat([Ravdess_df, Tess_df, Savee_df], axis = 0)\n",
    "\n",
    "# Shuffle the dataframe using the sample method\n",
    "aggregated_data = aggregated_data.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "# Drop rows where Emotions is 'fear' or 'disgust'\n",
    "#aggregated_data = aggregated_data[~aggregated_data['Emotions'].isin(['fear', 'disgust'])]\n",
    "\n",
    "# Drop rows where Emotions is \"sad\" and \"angry\" and replace them with \"unpleasant\"\n",
    "#aggregated_data = aggregated_data.drop(aggregated_data[aggregated_data['Emotions'] == 'sad'].sample(frac=0.4).index)\n",
    "#aggregated_data = aggregated_data.drop(aggregated_data[aggregated_data['Emotions'] == 'angry'].sample(frac=0.4).index)\n",
    "#aggregated_data['Emotions'] = aggregated_data['Emotions'].replace(['sad', 'angry'], 'unpleasant')\n",
    "\n",
    "aggregated_data.to_csv(\"data_path.csv\",index=False)\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotions\n",
       "neutral     808\n",
       "happy       652\n",
       "sad         652\n",
       "fear        652\n",
       "disgust     652\n",
       "surprise    652\n",
       "angry       652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.Emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotions</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/nirupamaunnithan/Downloads/Amvi/Trials/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotions                                               Path\n",
       "0         1  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "1         6  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "2         1  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "3         0  /Users/nirupamaunnithan/Downloads/Amvi/Trials/...\n",
       "4         1  /Users/nirupamaunnithan/Downloads/Amvi/Trials/..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {'neutral':0, 'happy':1, 'surprise':2, 'angry': 3, 'disgust':4, 'fear':5,'sad':6}\n",
    "aggregated_data.replace({'Emotions':labels},inplace=True)\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotions\n",
       "0    808\n",
       "1    652\n",
       "6    652\n",
       "5    652\n",
       "4    652\n",
       "2    652\n",
       "3    652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.Emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.5*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data: 0/4720\n",
      "Processing Data: 100/4720\n",
      "Processing Data: 200/4720\n",
      "Processing Data: 300/4720\n",
      "Processing Data: 400/4720\n",
      "Processing Data: 500/4720\n",
      "Processing Data: 600/4720\n",
      "Processing Data: 700/4720\n",
      "Processing Data: 800/4720\n",
      "Processing Data: 900/4720\n",
      "Processing Data: 1000/4720\n",
      "Processing Data: 1100/4720\n",
      "Processing Data: 1200/4720\n",
      "Processing Data: 1300/4720\n",
      "Processing Data: 1400/4720\n",
      "Processing Data: 1500/4720\n",
      "Processing Data: 1600/4720\n",
      "Processing Data: 1700/4720\n",
      "Processing Data: 1800/4720\n",
      "Processing Data: 1900/4720\n",
      "Processing Data: 2000/4720\n",
      "Processing Data: 2100/4720\n",
      "Processing Data: 2200/4720\n",
      "Processing Data: 2300/4720\n",
      "Processing Data: 2400/4720\n",
      "Processing Data: 2500/4720\n",
      "Processing Data: 2600/4720\n",
      "Processing Data: 2700/4720\n",
      "Processing Data: 2800/4720\n",
      "Processing Data: 2900/4720\n",
      "Processing Data: 3000/4720\n",
      "Processing Data: 3100/4720\n",
      "Processing Data: 3200/4720\n",
      "Processing Data: 3300/4720\n",
      "Processing Data: 3400/4720\n",
      "Processing Data: 3500/4720\n",
      "Processing Data: 3600/4720\n",
      "Processing Data: 3700/4720\n",
      "Processing Data: 3800/4720\n",
      "Processing Data: 3900/4720\n",
      "Processing Data: 4000/4720\n",
      "Processing Data: 4100/4720\n",
      "Processing Data: 4200/4720\n",
      "Processing Data: 4300/4720\n",
      "Processing Data: 4400/4720\n",
      "Processing Data: 4500/4720\n",
      "Processing Data: 4600/4720\n",
      "Processing Data: 4700/4720\n"
     ]
    }
   ],
   "source": [
    "NUM_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAMPLE_RATE = 22050\n",
    "DOWN_SAMPLE_RATE = 16000\n",
    "SAMPLE_NUM = aggregated_data.shape[0]\n",
    "\n",
    "data = {\n",
    "        \"labels\": [],\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "def extract_features(data, sample_rate):\n",
    "    mfcc = librosa.feature.mfcc(data, sample_rate, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    feature = mfcc.T\n",
    "    return feature\n",
    "\n",
    "for i in range(SAMPLE_NUM):\n",
    "    for j in range(2):\n",
    "        data['labels'].append(aggregated_data.iloc[i,0])\n",
    "    signal, sample_rate = librosa.load(aggregated_data.iloc[i,1], sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Cropping & Resampling\n",
    "    start_time = 0.4  # Start time in seconds\n",
    "    end_time = 1.9  # End time in seconds\n",
    "    start_frame = int(start_time * sample_rate)\n",
    "    end_frame = int(end_time * sample_rate)\n",
    "    signal = signal[start_frame:end_frame]\n",
    "    signal = librosa.resample(signal, sample_rate, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Add noise\n",
    "    signal = noise(signal)\n",
    "    res1 = extract_features(signal, DOWN_SAMPLE_RATE)\n",
    "    data[\"features\"].append(np.array(res1))\n",
    "    \n",
    "    # Stretch and shift pitch\n",
    "    new_data = stretch(signal)[:24000]\n",
    "    data_stretch_pitch = pitch(new_data, DOWN_SAMPLE_RATE)\n",
    "    res2 = extract_features(data_stretch_pitch, DOWN_SAMPLE_RATE)\n",
    "    data[\"features\"].append(np.array(res2))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f'Processing Data: {i}/{SAMPLE_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[72.08987572527357, 5.534708934733683, -1.377...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[67.86030970366848, 13.189012935613295, -10.5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-115.75637269257881, 27.472834745597417, 21....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-121.01478342470571, 37.319812902895094, 13....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-63.353990799337694, -5.711079993927296, 4.4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  labels\n",
       "0  [[72.08987572527357, 5.534708934733683, -1.377...       1\n",
       "1  [[67.86030970366848, 13.189012935613295, -10.5...       1\n",
       "2  [[-115.75637269257881, 27.472834745597417, 21....       6\n",
       "3  [[-121.01478342470571, 37.319812902895094, 13....       6\n",
       "4  [[-63.353990799337694, -5.711079993927296, 4.4...       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features = pd.DataFrame()\n",
    "Features['features'] = data[\"features\"]\n",
    "Features['labels'] = data[\"labels\"]\n",
    "Features.to_csv('Features.csv', index=False)\n",
    "Features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    1616\n",
       "1    1304\n",
       "6    1304\n",
       "5    1304\n",
       "4    1304\n",
       "2    1304\n",
       "3    1304\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(Features['features'])\n",
    "y = np.asarray(Features[\"labels\"])\n",
    "\n",
    "# Pad Features to make them of equal length\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:(6796, 47, 13) with label (6796,)\n",
      "Validate Data:(1700, 47, 13) with label (1700,)\n",
      " Testing Data:(944, 47, 13) with label (944,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print(f'Training Data:{X_train.shape} with label {y_train.shape}')\n",
    "print(f'Validate Data:{X_validation.shape} with label {y_validation.shape}')\n",
    "print(f' Testing Data:{X_test.shape} with label {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(64))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax')) # change the no of classes depending upon the no of labels\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (47,13)\n",
    "model = build_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m72,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,727</span> (495.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,727\u001b[0m (495.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,727</span> (495.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,727\u001b[0m (495.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimiser,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.2507 - loss: 1.8063 - val_accuracy: 0.4100 - val_loss: 1.4965\n",
      "Epoch 2/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.4643 - loss: 1.3805 - val_accuracy: 0.5294 - val_loss: 1.2156\n",
      "Epoch 3/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.5366 - loss: 1.1897 - val_accuracy: 0.5435 - val_loss: 1.1639\n",
      "Epoch 4/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.5980 - loss: 1.0698 - val_accuracy: 0.6153 - val_loss: 1.0272\n",
      "Epoch 5/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.6374 - loss: 0.9706 - val_accuracy: 0.6371 - val_loss: 0.9627\n",
      "Epoch 6/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.6490 - loss: 0.9196 - val_accuracy: 0.6353 - val_loss: 0.9709\n",
      "Epoch 7/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.6735 - loss: 0.8565 - val_accuracy: 0.6588 - val_loss: 0.9387\n",
      "Epoch 8/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.6791 - loss: 0.8158 - val_accuracy: 0.6394 - val_loss: 0.9622\n",
      "Epoch 9/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7040 - loss: 0.7679 - val_accuracy: 0.6606 - val_loss: 0.9136\n",
      "Epoch 10/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7120 - loss: 0.7627 - val_accuracy: 0.6612 - val_loss: 0.9008\n",
      "Epoch 11/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7231 - loss: 0.7185 - val_accuracy: 0.6859 - val_loss: 0.8798\n",
      "Epoch 12/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7332 - loss: 0.7159 - val_accuracy: 0.6729 - val_loss: 0.8844\n",
      "Epoch 13/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7363 - loss: 0.6780 - val_accuracy: 0.6765 - val_loss: 0.9050\n",
      "Epoch 14/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7377 - loss: 0.6549 - val_accuracy: 0.6759 - val_loss: 0.8868\n",
      "Epoch 15/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7472 - loss: 0.6512 - val_accuracy: 0.6853 - val_loss: 0.8494\n",
      "Epoch 16/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7545 - loss: 0.6307 - val_accuracy: 0.6806 - val_loss: 0.8860\n",
      "Epoch 17/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7558 - loss: 0.6409 - val_accuracy: 0.6782 - val_loss: 0.8827\n",
      "Epoch 18/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7683 - loss: 0.6140 - val_accuracy: 0.6771 - val_loss: 0.9227\n",
      "Epoch 19/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7737 - loss: 0.5935 - val_accuracy: 0.6871 - val_loss: 0.8707\n",
      "Epoch 20/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7786 - loss: 0.5808 - val_accuracy: 0.6859 - val_loss: 0.9214\n",
      "Epoch 21/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7925 - loss: 0.5413 - val_accuracy: 0.6994 - val_loss: 0.8803\n",
      "Epoch 22/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8069 - loss: 0.5189 - val_accuracy: 0.6918 - val_loss: 0.9165\n",
      "Epoch 23/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8084 - loss: 0.5242 - val_accuracy: 0.6918 - val_loss: 0.8996\n",
      "Epoch 24/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8028 - loss: 0.5192 - val_accuracy: 0.6894 - val_loss: 0.9301\n",
      "Epoch 25/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8152 - loss: 0.4975 - val_accuracy: 0.6747 - val_loss: 0.9780\n",
      "Epoch 26/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8095 - loss: 0.5114 - val_accuracy: 0.6929 - val_loss: 0.9407\n",
      "Epoch 27/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8231 - loss: 0.4753 - val_accuracy: 0.7006 - val_loss: 0.9542\n",
      "Epoch 28/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8218 - loss: 0.4772 - val_accuracy: 0.6947 - val_loss: 1.0106\n",
      "Epoch 29/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8431 - loss: 0.4321 - val_accuracy: 0.6718 - val_loss: 0.9919\n",
      "Epoch 30/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8379 - loss: 0.4477 - val_accuracy: 0.7024 - val_loss: 0.9475\n",
      "Epoch 31/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8434 - loss: 0.4225 - val_accuracy: 0.7035 - val_loss: 0.9659\n",
      "Epoch 32/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8470 - loss: 0.4103 - val_accuracy: 0.7035 - val_loss: 0.9809\n",
      "Epoch 33/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8584 - loss: 0.3762 - val_accuracy: 0.7024 - val_loss: 1.0186\n",
      "Epoch 34/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8505 - loss: 0.3942 - val_accuracy: 0.7094 - val_loss: 1.0456\n",
      "Epoch 35/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8590 - loss: 0.3806 - val_accuracy: 0.7006 - val_loss: 1.0353\n",
      "Epoch 36/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8671 - loss: 0.3616 - val_accuracy: 0.6906 - val_loss: 1.1229\n",
      "Epoch 37/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8491 - loss: 0.4102 - val_accuracy: 0.6953 - val_loss: 1.0417\n",
      "Epoch 38/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8821 - loss: 0.3292 - val_accuracy: 0.7018 - val_loss: 1.0890\n",
      "Epoch 39/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8675 - loss: 0.3500 - val_accuracy: 0.7041 - val_loss: 1.1193\n",
      "Epoch 40/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8789 - loss: 0.3352 - val_accuracy: 0.7041 - val_loss: 1.1647\n",
      "Epoch 41/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8932 - loss: 0.2897 - val_accuracy: 0.7147 - val_loss: 1.1255\n",
      "Epoch 42/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8751 - loss: 0.3451 - val_accuracy: 0.7153 - val_loss: 1.0977\n",
      "Epoch 43/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8854 - loss: 0.3117 - val_accuracy: 0.7029 - val_loss: 1.2095\n",
      "Epoch 44/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8814 - loss: 0.3082 - val_accuracy: 0.7012 - val_loss: 1.1936\n",
      "Epoch 45/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8923 - loss: 0.2819 - val_accuracy: 0.6941 - val_loss: 1.1757\n",
      "Epoch 46/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8841 - loss: 0.3300 - val_accuracy: 0.7053 - val_loss: 1.1572\n",
      "Epoch 47/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.9052 - loss: 0.2720 - val_accuracy: 0.7124 - val_loss: 1.2577\n",
      "Epoch 48/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.9118 - loss: 0.2481 - val_accuracy: 0.7035 - val_loss: 1.2173\n",
      "Epoch 49/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.9044 - loss: 0.2607 - val_accuracy: 0.6888 - val_loss: 1.2590\n",
      "Epoch 50/50\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.8953 - loss: 0.2963 - val_accuracy: 0.6929 - val_loss: 1.2385\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7108050847457628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_class = np.argmax(y_pred, axis=1)  \n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76       155\n",
      "           1       0.69      0.68      0.68       151\n",
      "           2       0.66      0.65      0.66       122\n",
      "           3       0.72      0.78      0.75       130\n",
      "           4       0.74      0.68      0.71       139\n",
      "           5       0.69      0.70      0.69       120\n",
      "           6       0.72      0.69      0.70       127\n",
      "\n",
      "    accuracy                           0.71       944\n",
      "   macro avg       0.71      0.71      0.71       944\n",
      "weighted avg       0.71      0.71      0.71       944\n",
      "\n",
      "Confusion Matrix:\n",
      "[[121   6   4   2   6   3  13]\n",
      " [  2 103  15  14   1  15   1]\n",
      " [  2  12  79   9   9   7   4]\n",
      " [  2   5   8 102   4   6   3]\n",
      " [ 10   8   3   8  95   6   9]\n",
      " [  5  11   4   6   7  84   3]\n",
      " [ 20   5   6   1   7   1  87]]\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.array(y_test)\n",
    "print(f\"Classification Report:\\n{classification_report(y_true, pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_true, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('Models/Speech-Emotion-Recognition-Model-1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('Models/Speech-Emotion-Recognition-Model-1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Cropping & Resampling\n",
    "    start_time = 0.4  # Start time in seconds\n",
    "    end_time = 1.9    # End time in seconds\n",
    "    start_frame = int(start_time * sample_rate)\n",
    "    end_frame = int(end_time * sample_rate)\n",
    "    signal = signal[start_frame:end_frame]\n",
    "    signal = librosa.resample(signal, sample_rate, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Add noise\n",
    "    signal = noise(signal)\n",
    "    res1 = extract_features(signal, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Stretch and shift pitch\n",
    "    new_data = stretch(signal)[:24000]\n",
    "    data_stretch_pitch = pitch(new_data, DOWN_SAMPLE_RATE)\n",
    "    res2 = extract_features(data_stretch_pitch, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Prepare input by padding\n",
    "    X_input = [res1, res2]\n",
    "    X_input = tf.keras.preprocessing.sequence.pad_sequences(X_input)\n",
    "    \n",
    "    return np.array(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "Predicted Emotion: sad\n"
     ]
    }
   ],
   "source": [
    "# Path to your new audio file\n",
    "file_path = 'TESS Toronto emotional speech set data/OAF_Sad/OAF_bean_sad.wav'\n",
    "\n",
    "# Preprocess the new audio file\n",
    "X_input = preprocess_audio(file_path)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_input)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the predicted emotion label\n",
    "#emotion_labels = [\"Neutral\", \"Calm\", \"Happy\", \"Sad\", \"Angry\", \"Fearful\", \"Disgust\", \"Surprised\"]\n",
    "emotion_labels = [\"neutral\", \"happy\", \"surprise\", \"angry\", \"disgust\", \"fear\", \"sad\"]\n",
    "print(f\"Predicted Emotion: {emotion_labels[predicted_label[0]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 47, 13), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13390839504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13350674752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13390836336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399947072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399947424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399933872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400016496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400015968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400026704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400027584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1726726184.885719  827713 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1726726184.885729  827713 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-19 11:39:44.885849: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3\n",
      "2024-09-19 11:39:44.886591: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-19 11:39:44.886596: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3\n",
      "2024-09-19 11:39:44.894638: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-19 11:39:44.929109: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmp7_rr3oq3\n",
      "2024-09-19 11:39:44.944358: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 58508 microseconds.\n",
      "2024-09-19 11:39:45.001658: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3463] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<i32>, tensor<?x128xf32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<2xi32>) -> (tensor<47x?x128xf32>) : {device = \"\", num_elements = 47 : i64}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 47, 13), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13390839504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13350674752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13390836336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399947072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399947424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13399933872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400016496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400015968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400026704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13400027584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1726726185.335481  827713 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1726726185.335494  827713 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-09-19 11:39:45.335614: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7\n",
      "2024-09-19 11:39:45.336256: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-09-19 11:39:45.336261: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7\n",
      "2024-09-19 11:39:45.344335: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-09-19 11:39:45.378084: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/j1/1n5_k0ss3w38z0f3qkzrynj80000gn/T/tmpck2qnvw7\n",
      "2024-09-19 11:39:45.393548: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 57933 microseconds.\n",
      "2024-09-19 11:39:45.459137: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3463] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<i32>, tensor<?x128xf32>) -> (tensor<!tf_type.variant<tensor<?x128xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x128xf32>>>, tensor<2xi32>) -> (tensor<47x?x128xf32>) : {device = \"\", num_elements = 47 : i64}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<1x?x64xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False  # Disable lowering of tensor list ops\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with tf.io.gfile.GFile(\"Models/Speech_Emo_Rec.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quant_tflite_model = converter.convert()\n",
    "with tf.io.gfile.GFile(\"Models/Speech_Emo_Rec_quant.tflite\", 'wb') as f:\n",
    "   f.write(quant_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Sizes:\n",
      "\t\n",
      "514K\tSpeech_Emo_Rec.tflite\n",
      "165K\tSpeech_Emo_Rec_quant.tflite\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Sizes:\")\n",
    "!ls -lh Models | awk '{print $5 \"\\t\" $9}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 11:39:54.467420: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_inter_op_parallelism which is not in the op definition: Op<name=TensorListReserve; signature=element_shape:shape_type, num_elements:int32 -> handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node TensorListReserve}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Lite quant Model Accuracy: 71.61%\n",
      "TF Lite Model Accuracy: 71.08%\n",
      "\n",
      "Original Accuracy  :   0.7108050847457628\n",
      "Accuracy Difference from Original Model of Speech_Emo_Rec_quant.tflite : -0.53%\n",
      "Accuracy Difference from Original Model of Speech_Emo_Rec.tflite : 0.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tflite(interpreter, test_data, test_label):\n",
    "    # Get the input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Iterate over the testing data.\n",
    "    for i in range(test_data.shape[0]):\n",
    "        # Get the input data for this example.\n",
    "        input_data = np.array([test_data[i]], dtype=np.float32)\n",
    "\n",
    "        # Set the input tensor.\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor.\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # Compute the predicted label.\n",
    "        predicted_label = np.argmax(output_data)\n",
    "\n",
    "        # Update the results.\n",
    "        if predicted_label == test_label[i]:\n",
    "            num_correct += 1\n",
    "        num_total += 1\n",
    "\n",
    "    # Reset all variables so it will not pollute other inferences.\n",
    "    interpreter.reset_all_variables()\n",
    "    \n",
    "    # Compute the accuracy.\n",
    "    accuracy = num_correct / num_total\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "# Load tflite model.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"Models/Speech_Emo_Rec_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "tflite_test_acc = evaluate_tflite(interpreter, X_test, y_test)\n",
    "print(f\"TF Lite quant Model Accuracy: {tflite_test_acc * 100:.2f}%\")\n",
    "\n",
    "# Load Speech_Emo_Rec.tflite model\n",
    "interpreter2 = tf.lite.Interpreter(model_path=\"Models/Speech_Emo_Rec.tflite\")\n",
    "interpreter2.allocate_tensors()\n",
    "\n",
    "tflite_test_acc2 = evaluate_tflite(interpreter2, X_test, y_test)\n",
    "print(f\"TF Lite Model Accuracy: {tflite_test_acc2 * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nOriginal Accuracy  :  \", accuracy)\n",
    "print(f\"Accuracy Difference from Original Model of Speech_Emo_Rec_quant.tflite : {(accuracy-tflite_test_acc) * 100:.2f}%\")\n",
    "print(f\"Accuracy Difference from Original Model of Speech_Emo_Rec.tflite : {(accuracy-tflite_test_acc2) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 11:40:00.127405: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_inter_op_parallelism which is not in the op definition: Op<name=TensorListReserve; signature=element_shape:shape_type, num_elements:int32 -> handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node TensorListReserve}}\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"Models/Speech_Emo_Rec_quant.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_input_layer_1:0', 'index': 0, 'shape': array([ 1, 47, 13], dtype=int32), 'shape_signature': array([-1, 47, 13], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(input_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'StatefulPartitionedCall_1:0', 'index': 48, 'shape': array([1, 7], dtype=int32), 'shape_signature': array([-1,  7], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path):\n",
    "    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Cropping & Resampling\n",
    "    start_time = 0.4  # Start time in seconds\n",
    "    end_time = 1.9    # End time in seconds\n",
    "    start_frame = int(start_time * sample_rate)\n",
    "    end_frame = int(end_time * sample_rate)\n",
    "    signal = signal[start_frame:end_frame]\n",
    "    signal = librosa.resample(signal, sample_rate, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Add noise and extract features\n",
    "    signal = noise(signal)\n",
    "    res1 = extract_features(signal, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Stretch and shift pitch\n",
    "    new_data = stretch(signal)[:24000]\n",
    "    data_stretch_pitch = pitch(new_data, DOWN_SAMPLE_RATE)\n",
    "    res2 = extract_features(data_stretch_pitch, DOWN_SAMPLE_RATE)\n",
    "    \n",
    "    # Prepare input by padding\n",
    "    X_input = [res1, res2]\n",
    "    X_input = tf.keras.preprocessing.sequence.pad_sequences(X_input)\n",
    "    \n",
    "    return np.array(X_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your new audio file\n",
    "file_path = 'TESS Toronto emotional speech set data/OAF_Fear/OAF_chair_fear.wav'\n",
    "\n",
    "# Preprocess the new audio file\n",
    "X_input = preprocess_audio(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 47, 13)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 47, 13)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "X_reshaped = np.mean(X_input, axis=0).reshape(1, 47, 13).astype(np.float32)\n",
    "print(X_reshaped.shape)\n",
    "print(X_reshaped.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], X_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: fear\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted output\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = np.argmax(output_data)\n",
    "\n",
    "# Print the predicted emotion label\n",
    "emotion_labels = [\"neutral\", \"happy\", \"surprise\", \"angry\", \"disgust\", \"fear\", \"sad\"]\n",
    "print(f\"Predicted Emotion: {emotion_labels[predicted_label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
